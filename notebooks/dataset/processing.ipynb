{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing dataset\n",
    "\n",
    "Observe that this notebook aims to illustrate the process of dataset processing to perform experiments on cycle-WGAN. Therefore, we show how to use the splits proposed by Xian et al [1], on the datasets CUB, AWA1 and SUN. We also provided the h5files for CUB, SUN, FLO and AWA1 at the end of this page (if required to perform your experiments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "```\n",
    "jupyter                   1.0.0\n",
    "\n",
    "jupyter_client            5.2.3\n",
    "\n",
    "jupyter_console           5.2.0\n",
    "\n",
    "jupyter_core              4.4.0\n",
    "\n",
    "git clone https://github.com/rfelixmg/util.git\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing dependencies...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('../../'))\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "sys.path.append(os.path.abspath('./'))\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from util.datasets import load_h5\n",
    "from util.storage import Container, DataH5py\n",
    "\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "print(\"importing dependencies...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing directories\n",
      "mkdir: cannot create directory ‘../../data/cub’: File exists\n",
      "mkdir: cannot create directory ‘../../data/awa1’: File exists\n",
      "mkdir: cannot create directory ‘../../data/sun’: File exists\n",
      "mkdir: cannot create directory ‘../../data/._original_’: File exists\n",
      "File `../../data/._original_/xian.etal.zip' already there; not retrieving.\n",
      "mv: cannot move '../../data/._original_/xlsa17/code' to '../../data/._original_/code': Directory not empty\n",
      "mv: cannot move '../../data/._original_/xlsa17/data' to '../../data/._original_/data': Directory not empty\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing directories\")\n",
    "!mkdir ../../data/cub ../../data/awa1 ../../data/sun\n",
    "!mkdir ../../data/._original_\n",
    "\n",
    "!wget -nc http://datasets.d2.mpi-inf.mpg.de/xian/xlsa17.zip -O ../../data/._original_/xian.etal.zip\n",
    "!unzip -u -n -q ../../data/._original_/xian.etal.zip -d ../../data/._original_/ \n",
    "!mv ../../data/._original_/xlsa17/* ../../data/._original_/\n",
    "!wget -q -nc https://www.dropbox.com/s/7yf0b1cx900ardo/cub_attributes_reed.npy?dl=0 -O ../../data/._original_/data/CUB/reed.etal.npy\n",
    "#!ls ../../data/._original_/\n",
    "\n",
    "_download_folder_ = '../../data/._original_/'\n",
    "_outdir = '../../data/'\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dname, basedir, has_semantic=False):\n",
    "    from scipy.io import loadmat\n",
    "    import numpy as np\n",
    "    from util.storage import DictContainer, Container\n",
    "\n",
    "    _dir_src = '{}/data/{}/'.format(basedir, dname)\n",
    "    \n",
    "    # For more information, pelase check Xian et al [1];\n",
    "    _all_splits = loadmat('{}/{}'.format(_dir_src, 'att_splits.mat'))\n",
    "    _odata = loadmat('{}/data/{}/res101.mat'.format(basedir, dname))\n",
    "    \n",
    "    # Verify whether features the \\in R^(NxD) or R^(DxN), \n",
    "    # where N is number of samples\n",
    "    if _odata['features'].shape[0] == 2048:\n",
    "        _odata['features'] = _odata['features'].T\n",
    "    # from 0 to (N-1)\n",
    "    all_ids = np.arange(0, _odata['features'].shape[0])\n",
    "    \n",
    "    # Labels from 0 to (|C|-1)\n",
    "    _odata['y'] = _odata['labels'] - 1\n",
    "    \n",
    "    # For more information, pelase check Xian et al [1];\n",
    "    if has_semantic != False:\n",
    "        # If semantic space is different then Xian et al [1];\n",
    "        att_continuous = np.load(has_semantic)\n",
    "    else:\n",
    "        att_continuous = _all_splits['att'].T\n",
    "\n",
    "    # Although we don't use the binary attributes, you can easily compute then following [2]\n",
    "    binary_attributes = (att_continuous >= att_continuous.mean()).astype(np.float)\n",
    "\n",
    "    # Sorting semantic features by class label\n",
    "    all_continuous_attributes = np.array([att_continuous[_label] for _label in _odata['y']])\n",
    "    all_binary_attributes = np.array([binary_attributes[_label] for _label in _odata['y']])\n",
    "    \n",
    "    # naming by Xian et al [1]\n",
    "    _sets = ['train_loc', 'val_loc', 'trainval_loc', 'test_seen_loc', 'test_unseen_loc']\n",
    "    # our naming for h5Files\n",
    "    _namespaces = ['train_val', 'val', 'train', 'test/seen', 'test/unseen']\n",
    "    \n",
    "    ndb = DictContainer()\n",
    "    for _set, _name in zip(_sets, _namespaces):\n",
    "        \n",
    "        #print(_set, _name)\n",
    "        # as labels go from 1 to |C|, we need to subtract 1\n",
    "        _ids = _all_splits[_set] - 1\n",
    "        ndb.set_param('{}/A/continuous'.format(_name), all_continuous_attributes[_ids].squeeze())\n",
    "        ndb.set_param('{}/A/binary'.format(_name), all_binary_attributes[_ids].squeeze())\n",
    "        ndb.set_param('{}/X'.format(_name), _odata['features'][_ids].squeeze())\n",
    "        ndb.set_param('{}/Y'.format(_name), _odata['labels'][_ids].squeeze())\n",
    "        ndb.set_param('{}/y'.format(_name), _odata['y'][_ids].squeeze())\n",
    "    ndb = Container(ndb.as_dict())\n",
    "    ndb.name = dname\n",
    "    if has_semantic != False:\n",
    "        ndb.semantic = has_semantic\n",
    "    return ndb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_knn(dname, basedir, has_semantic=False):\n",
    "    from scipy.io import loadmat\n",
    "    import numpy as np\n",
    "    from util.storage import DictContainer, Container\n",
    "\n",
    "    _dir_src = '{}/data/{}/'.format(basedir, dname)\n",
    "    \n",
    "    # For more information, pelase check Xian et al [1];\n",
    "    _all_splits = loadmat('{}/{}'.format(_dir_src, 'att_splits.mat'))\n",
    "    _odata = loadmat('{}/data/{}/res101.mat'.format(basedir, dname))\n",
    "    \n",
    "    # Labels from 0 to (|C|-1)\n",
    "    _odata['y'] = _odata['labels'].squeeze() - 1\n",
    "    \n",
    "    # For more information, pelase check Xian et al [1];\n",
    "    if has_semantic != False:\n",
    "        # If semantic space is different then Xian et al [1];\n",
    "        att_continuous = np.load(has_semantic)\n",
    "    else:\n",
    "        att_continuous = _all_splits['att'].T\n",
    "    \n",
    "    nknn = DictContainer()\n",
    "    def _setting_domain_(_domain, _classes):\n",
    "        nknn.set_param('{}/data'.format(_domain), \n",
    "                       np.array([att_continuous[i] for i in _classes]))\n",
    "        nknn.set_param('{}/ids'.format(_domain), _classes + 1)\n",
    "        nknn.set_param('{}/ys'.format(_domain), _classes)\n",
    "        nknn.set_param('{}/labels'.format(_domain), \n",
    "                       np.array([_all_splits['allclasses_names'][i][0][0] for i in _classes]))\n",
    "        \n",
    "        nknn.set_param('{}/knn2id'.format(_domain), \n",
    "                       {key: value for key, value in enumerate(nknn.get_param('{}/ids'.format(_domain)))})\n",
    "        nknn.set_param('{}/id2knn'.format(_domain), \n",
    "                       {value: key for key, value in enumerate(nknn.get_param('{}/ids'.format(_domain)))})\n",
    "        nknn.set_param('{}/id2class'.format(_domain), \n",
    "        {value: nknn.get_param('{}/labels'.format(_domain))[key] for key, value in enumerate(nknn.get_param('{}/ids'.format(_domain)))})\n",
    "\n",
    "        \n",
    "    _setting_domain_('openset', np.arange(0, att_continuous.shape[0]))\n",
    "    \n",
    "    _classes = np.unique(_odata['y'][_all_splits['trainval_loc'] - 1].squeeze())\n",
    "    _setting_domain_('openval', _classes)\n",
    "    \n",
    "    _classes = np.unique(_odata['y'][_all_splits['test_unseen_loc'] - 1].squeeze())\n",
    "    _setting_domain_('zsl', _classes)\n",
    "\n",
    "    return nknn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared: awa1\n",
      "KNN prepared: awa1\n",
      "Data saved\n"
     ]
    }
   ],
   "source": [
    "_dname = 'AWA1'\n",
    "_data = prepare_dataset(dname=_dname, basedir=_download_folder_)\n",
    "print(\"Data prepared: {}\".format(_dname.lower()))\n",
    "\n",
    "_knn = prepare_knn(dname=_dname, basedir=_download_folder_)\n",
    "print(\"KNN prepared: {}\".format(_dname.lower()))\n",
    "\n",
    "\n",
    "DataH5py().save(dic=_data, filename='{}/{}/data.h5'.format(_outdir, _dname.lower()))\n",
    "DataH5py().save(dic=_knn, filename='{}/{}/knn.h5'.format(_outdir, _dname.lower()))\n",
    "print(\"Data saved\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepare: cub\n",
      "KNN prepared: cub\n",
      "Data saved\n"
     ]
    }
   ],
   "source": [
    "_dname = 'CUB'\n",
    "\n",
    "# For more information, pelase check Reed et al [3];\n",
    "_semantic_dir = '../../data/._original_/data/CUB/reed.etal.npy'\n",
    "\n",
    "_data = prepare_dataset(dname=_dname, basedir=_download_folder_, has_semantic=_semantic_dir)\n",
    "print(\"Data prepare: {}\".format(_dname.lower()))\n",
    "\n",
    "_knn = prepare_knn(dname=_dname, basedir=_download_folder_, has_semantic=_semantic_dir)\n",
    "print(\"KNN prepared: {}\".format(_dname.lower()))\n",
    "\n",
    "\n",
    "DataH5py().save(dic=_data, filename='{}/{}/data.h5'.format(_outdir, _dname.lower()))\n",
    "DataH5py().save(dic=_knn, filename='{}/{}/knn.h5'.format(_outdir, _dname.lower()))\n",
    "print(\"Data saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared: sun\n",
      "KNN prepared: sun\n",
      "Data saved\n"
     ]
    }
   ],
   "source": [
    "_dname = 'SUN'\n",
    "_data = prepare_dataset(dname=_dname, basedir=_download_folder_)\n",
    "print(\"Data prepared: {}\".format(_dname.lower()))\n",
    "\n",
    "_knn = prepare_knn(dname=_dname, basedir=_download_folder_)\n",
    "print(\"KNN prepared: {}\".format(_dname.lower()))\n",
    "\n",
    "\n",
    "DataH5py().save(dic=_data, filename='{}/{}/data.h5'.format(_outdir, _dname.lower()))\n",
    "DataH5py().save(dic=_knn, filename='{}/{}/knn.h5'.format(_outdir, _dname.lower()))\n",
    "print(\"Data saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download H5files\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to guarantee reproducibility of our approach, with numbers reported on Felix et al [4]. We attached to this file the weblinks to (1) download the h5files used to performing the training of models. (2) semantic space used for CUB; and (3) the pseudo-features generated in order to train the final classifier;\n",
    "\n",
    "\n",
    "(1) Dataset (h5file):\n",
    "https://drive.google.com/open?id=1wUNnqdNTapl7fZsGczUZIlQnGkpAtTpv\n",
    "\n",
    "(2) CUB semantic space:\n",
    "https://drive.google.com/open?id=1Jfltw0qbSCUvr4ekRl_3pekkCGzGolyA\n",
    "\n",
    "(3) Pseudo features generated by cycle-WGAN:\n",
    "https://drive.google.com/open?id=1MTGiOL3rbW6vi-WeTKbdix1hHcq6i5ki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Xian, Yongqin, Bernt Schiele, and Zeynep Akata. \"Zero-shot learning-the good, the bad and the ugly.\" arXiv preprint arXiv:1703.04394 (2017).\n",
    "\n",
    "[2] Lampert, Christoph H., Hannes Nickisch, and Stefan Harmeling. \"Learning to detect unseen object classes by between-class attribute transfer.\" Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, 2009.\n",
    "\n",
    "[3] Reed, Scott, et al. \"Learning deep representations of fine-grained visual descriptions.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\n",
    "\n",
    "[4] Felix, Rafael, et al. \"Multi-modal Cycle-consistent Generalized Zero-Shot Learning.\" Proceedings of the European Conference on Computer Vision (ECCV). 2018.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
